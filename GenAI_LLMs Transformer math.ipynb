{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I'll illustrate how to fine-tune a Hugging Face model and generate text using transformers.\n",
        "\n",
        "Transformers are a type of model architecture that has revolutionized NLP tasks. They use self-attention mechanisms to process input data in parallel, making them efficient for large datasets.\n"
      ],
      "metadata": {
        "id": "gCnrBkPcs_aQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Underlying Math\n",
        "\n",
        "The mathematics behind the transformer model, particularly the self-attention mechanism. This is the core idea that powers generative models like GPT-2 and GPT-3.\n",
        "\n",
        "**Self-Attention Mechanism**\n",
        "\n",
        "The **self-attention mechanism** allows the model to attend to different parts of the input sequence. It works by computing three vectors for each token in the sequence:\n",
        "- **Query (Q)**: Represents the token's search request.\n",
        "- **Key (K)**: Represents the token's content.\n",
        "- **Value (V)**: Holds the token's information.\n",
        "\n",
        "These vectors are calculated by multiplying the input embeddings $X$ with learned weight matrices:\n",
        "$\n",
        "Q = X W_Q, \\quad K = X W_K, \\quad V = X W_V\n",
        "$\n",
        "\n",
        "where:\n",
        "- $X$ is the input matrix of embeddings.\n",
        "- $ W_Q, W_K, W_V$ are learnable weight matrices.\n",
        "\n",
        "**Scaled Dot-Product Attention**\n",
        "\n",
        "Once we have these vectors, the model computes how much attention each token should give to others by taking the **dot product** of the Query and Key vectors. This gives an initial attention score:\n",
        "\n",
        "$\n",
        "\\text{Attention Score} = Q \\cdot K^T\n",
        "$\n",
        "\n",
        "This score is then **scaled** by $ \\sqrt{d_k} $, where $d_k$ is the dimension of the key vector. This scaling ensures that the dot product values aren’t too large, which could destabilize the training process:\n",
        "\n",
        "$\n",
        "\\text{Scaled Score} = \\frac{Q \\cdot K^T}{\\sqrt{d_k}}\n",
        "$\n",
        "\n",
        "The **softmax** function is applied to convert these scores into probabilities (attention weights), so that they sum to 1:\n",
        "\n",
        "$\n",
        "\\text{Attention Weights} = \\text{softmax}\\left( \\frac{Q \\cdot K^T}{\\sqrt{d_k}} \\right)\n",
        "$\n",
        "\n",
        "\n",
        "These attention weights are used to take a **weighted sum** of the Value vectors, resulting in the **output** of the self-attention layer:\n",
        "\n",
        "$\n",
        "\\text{Output} = \\text{Attention Weights} \\cdot V\n",
        "$\n",
        "\n",
        "**Multi-Head Attention**\n",
        "\n",
        "To allow the model to focus on different relationships in the input simultaneously, **multi-head attention** is used. Instead of computing attention once, the model computes it multiple times in parallel, using different sets of learned weights for each head. The outputs of all attention heads are concatenated and projected into the final output:\n",
        "\n",
        "$\n",
        "Q_i = X W_Q^i, \\quad K_i = X W_K^i, \\quad V_i = X W_V^i\n",
        "$\n",
        "\n",
        "The multi-head attention output is:\n",
        "\n",
        "$$\n",
        "\\text{Multi-Head Output} = \\text{concat}(\\text{head}_1, \\text{head}_2, \\dots, \\text{head}_h) W_O\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $h$ is the number of attention heads.\n",
        "- $W_O$ is a projection matrix that combines the attention heads.\n",
        "\n",
        "**Positional Encoding**\n",
        "\n",
        "Since transformers process tokens in parallel, they don’t inherently know the order of tokens. To address this, **positional encoding** is added to the input embeddings to provide order information. The positional encoding is computed using sine and cosine functions:\n",
        "\n",
        "$\n",
        "PE_{(i, 2k)} = \\sin\\left(\\frac{i}{10000^{2k/d}}\\right), \\quad PE_{(i, 2k+1)} = \\cos\\left(\\frac{i}{10000^{2k/d}}\\right)\n",
        "$\n",
        "\n",
        "where:\n",
        "- $ i$ is the position of the token in the sequence.\n",
        "- $d$ is the dimensionality of the token embeddings.\n",
        "\n",
        "These positional encodings are added to the input embeddings to provide the model with information about the relative position of tokens in the sequence.\n",
        "\n",
        "**Why Self-Attention?**\n",
        "\n",
        "Self-attention is crucial for allowing transformers to handle long-range dependencies in text. By processing all tokens at once and allowing the model to decide which tokens to focus on, it can capture relationships between tokens that are far apart in the sequence. This ability to \"pay attention\" to the important parts of the input is what makes transformers so powerful for **text generation** and other natural language tasks.\n"
      ],
      "metadata": {
        "id": "Bzh1VOZ5e4Qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pyton code:\n",
        "\n",
        "- Fine-tune a pre-trained model on your dataset.\n",
        "\n",
        "- Use the fine-tuned model to generate text based on a given prompt.\n"
      ],
      "metadata": {
        "id": "t9f1iSAViH8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 1: Setting Up Your Environment\n",
        "\n",
        "Install Hugging Face Transformers library along with PyTorch or TensorFlow (depending on your preference)."
      ],
      "metadata": {
        "id": "VpXcK_V_s_lM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch  # For PyTorch"
      ],
      "metadata": {
        "id": "2xHaVk2LtURc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "EbTTL3e5ySzi"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1n0aXFR4bRK",
        "outputId": "41f322f7-dc08-4a44-c7e3-7c2325d5c0ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.51.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n"
      ],
      "metadata": {
        "id": "HoQGQRns4ilA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 2: Loading a Pre-trained Model\n",
        "\n",
        "Hugging Face provides a wide range of pre-trained models. Here’s how to load a pre-trained model and tokenizer for text generation.\n"
      ],
      "metadata": {
        "id": "qQ16cALMYWov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can also try \"gpt2-medium\", \"gpt2-large\", etc.\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4SzqTYeYeZa",
        "outputId": "910d51f2-15fd-4af0-edd8-5c7baa65cd6d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GPT2LMHeadModel(\n",
              "  (transformer): GPT2Model(\n",
              "    (wte): Embedding(50257, 768)\n",
              "    (wpe): Embedding(1024, 768)\n",
              "    (drop): Dropout(p=0.1, inplace=False)\n",
              "    (h): ModuleList(\n",
              "      (0-11): 12 x GPT2Block(\n",
              "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (attn): GPT2Attention(\n",
              "          (c_attn): Conv1D(nf=2304, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=768)\n",
              "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
              "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        (mlp): GPT2MLP(\n",
              "          (c_fc): Conv1D(nf=3072, nx=768)\n",
              "          (c_proj): Conv1D(nf=768, nx=3072)\n",
              "          (act): NewGELUActivation()\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 3: Generating Text\n",
        "\n",
        "Now that you have the model and tokenizer, you can generate text. Here’s a simple example:"
      ],
      "metadata": {
        "id": "p1-BYRjpaB9a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to generate text\n",
        "def generate_text(prompt, max_length=50):\n",
        "    # Encode the input prompt\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
        "\n",
        "    # Generate text\n",
        "    with torch.no_grad():\n",
        "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1)\n",
        "\n",
        "    # Decode the generated text\n",
        "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    return generated_text\n",
        "\n",
        "# Example usage\n",
        "prompt = \"The individual has always had to struggle to keep from being overwhelmed by the tribe. If you try it\"\n",
        "generated = generate_text(prompt)\n",
        "print(generated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XNCTRUBZaEZZ",
        "outputId": "89f4631b-3a69-437d-f3a3-ef145882da4c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The individual has always had to struggle to keep from being overwhelmed by the tribe. If you try it, you'll be able to get through it.\n",
            "\n",
            "\"I think it's a very important thing to have a good relationship with your tribe.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next: Fine-tuning a Model on a Dataset\n"
      ],
      "metadata": {
        "id": "fKW6eSTCYexn"
      }
    }
  ]
}